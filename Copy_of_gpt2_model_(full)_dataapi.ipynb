{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "17ed0b92",
      "metadata": {
        "id": "17ed0b92"
      },
      "source": [
        "\n",
        "# DS6050 - Group 6\n",
        "* Andrej Erkelens <wsw3fa@virginia.edu>\n",
        "* Robert Knuuti <uqq5zz@virginia.edu>\n",
        "* Khoi Tran <kt2np@virginia.edu>\n",
        "\n",
        "## Abstract\n",
        "English is a verbose language with over 69% redundancy in its construction, and as a result, individuals only need to identify important details to comprehend an intended message.\n",
        "While there are strong efforts to quantify the various elements of language, the average individual can still comprehend a written message that has errors, either in spelling or in grammar.\n",
        "The emulation of the effortless, yet obscure task of reading, writing, and understanding language is the perfect challenge for the biologically-inspired methods of deep learning.\n",
        "Most language and text related problems rely upon finding high-quality latent representations to understand the task at hand. Unfortunately, efforts to overcome such problems are limited to the data and computation power available to individuals; data availability often presents the largest problem, with small, specific domain tasks often proving to be limiting.\n",
        "Currently, these tasks are often aided or overcome by pre-trained large language models (LLMs), designed by large corporations and laboratories.\n",
        "Fine-tuning language models on domain-specific vocabulary with small data sizes still presents a challenge to the language community, but the growing availability of LLMs to augment such models alleviates the challenge.\n",
        "This paper explores different techniques to be applied on existing language models (LMs), built highly complex Deep Learning models, and investigates how to fine-tune these models, such that a pre-trained model is used to enrich a more domain-specific model that may be limited in textual data.\n",
        "\n",
        "## Project Objective\n",
        "\n",
        "We are aiming on using several small domain specific language tasks, particularly classification tasks.\n",
        "We aim to take at least two models, probably BERT and distill-GPT2 as they seem readily available on HuggingFace and TensorFlow's model hub.\n",
        "We will iterate through different variants of layers we fine tune and compare these results with fully trained models, and ideally find benchmarks already in academic papers on all of the datasets.\n",
        "\n",
        "We aim to optimize compute efficiency and also effectiveness of the model on the given dataset. Our goal is to find a high performing and generalizable method for our fine tuning process and share this in our paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8971652",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f8971652",
        "outputId": "cf33b1a6-447c-499a-e283-8feaeda3db69"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(0)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autosave disabled\n"
          ]
        }
      ],
      "source": [
        "%autosave 0\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7fV1R567PdR",
      "metadata": {
        "id": "M7fV1R567PdR"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    %pip install -q tensorflow-addons tensorflow-text tokenizers transformers\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    %cd /content/drive/MyDrive/ds6050/git/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8606fc-267c-4469-b40e-341cc560d102",
      "metadata": {
        "id": "bf8606fc-267c-4469-b40e-341cc560d102"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb2e492",
      "metadata": {
        "id": "6eb2e492",
        "outputId": "362d9de5-7f99-4458-8250-e0770ec6d38d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-09 00:58:36.747226: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "/home/uqq5zz/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2022-08-09 00:58:41.345799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-08-09 00:58:43.448910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30902 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:18:00.0, compute capability: 7.0\n",
            "2022-08-09 00:58:43.450165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30965 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
            "2022-08-09 00:58:43.451741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30965 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
            "2022-08-09 00:58:43.453273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30965 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt, nltk, numpy as np, os, \\\n",
        "       pandas as pd, re, seaborn as sns, string, tokenizers, \\\n",
        "       tensorflow as tf, tensorflow_addons as tfa, \\\n",
        "       tensorflow_text as tf_text, torch, transformers\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from tensorflow import keras\n",
        "from tokenizers import decoders, models, normalizers, \\\n",
        "                       pre_tokenizers, processors, trainers\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "df = pd.read_feather(\"data/dataset.feather\")#.set_index('index')\n",
        "df['topic'] = df['topic'].str.split('.').str[0]\n",
        "df_train = df.sample(frac = 0.8)\n",
        "df_test = df.drop(df_train.index)\n",
        "\n",
        "features = 'content' # feature for the future - add all the datasets ['categories', 'summary', 'content']\n",
        "label = 'topic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k98DISb3X2Yr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k98DISb3X2Yr",
        "outputId": "05774065-7efc-40f7-8874-8373627e19f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "# 7 different topics\n",
        "# y_ = ohe.fit_transform(df['topic'].values.reshape(-1,1)).toarray()\n",
        "y_train = ohe.fit_transform(df_train['topic'].values.reshape(-1,1)).toarray()\n",
        "y_test  = ohe.fit_transform(df_test['topic'].values.reshape(-1,1)).toarray()\n",
        "\n",
        "max_len = 512\n",
        "checkpoint = 'gpt2'\n",
        "hf_gpt2_tokenizer = transformers.GPT2TokenizerFast.from_pretrained(checkpoint, add_prefix_space=True)\n",
        "# hf_gpt2_model = transformers.GPT2ForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# add for gpt2 padding\n",
        "if hf_gpt2_tokenizer.pad_token is None:\n",
        "    hf_gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "## create model\n",
        "def model_top(pretr_model):\n",
        "    with strategy.scope():\n",
        "        input_ids = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
        "        attention_mask = tf.keras.Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "        output = pretr_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        # output = pretr_model([input_ids, attention_mask])\n",
        "        #pooler_output = output[1]\n",
        "        pooler_output = tf.keras.layers.AveragePooling1D(pool_size=max_len)(output[0])\n",
        "        flattened_output = tf.keras.layers.Flatten()(pooler_output)\n",
        "\n",
        "        output = tf.keras.layers.Dense(max_len, activation='relu')(flattened_output)\n",
        "        #output = tf.keras.layers.Dropout(0.2)(output)\n",
        "        output = tf.keras.layers.Dropout(0.4)(output)\n",
        "\n",
        "        output = tf.keras.layers.Dense(7, activation='softmax')(output)\n",
        "        model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "train_encodings = hf_gpt2_tokenizer.batch_encode_plus(list(df_train.summary.values), \n",
        "                                                      return_tensors='tf', \n",
        "                                                      padding='max_length',\n",
        "                                                      max_length = max_len,\n",
        "                                                      truncation=True)\n",
        "\n",
        "test_encodings  = hf_gpt2_tokenizer.batch_encode_plus(list(df_test.summary.values), \n",
        "                                                      return_tensors='tf', \n",
        "                                                      padding='max_length',\n",
        "                                                      max_length = max_len,\n",
        "                                                      truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XaMC57JY4YRv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaMC57JY4YRv",
        "outputId": "d5349c8f-6d36-430c-9c4a-7f98a09f5fe2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tfgpt2_model (TFGPT2Model)     TFBaseModelOutputWi  124440576   ['input_1[0][0]',                \n",
            "                                thPastAndCrossAtten               'input_2[0][0]']                \n",
            "                                tions(last_hidden_s                                               \n",
            "                                tate=(None, 512, 76                                               \n",
            "                                8),                                                               \n",
            "                                 past_key_values=((                                               \n",
            "                                2, None, 12, 512, 6                                               \n",
            "                                4),                                                               \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64)),                                                            \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None,                                                \n",
            "                                cross_attentions=No                                               \n",
            "                                ne)                                                               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 1, 768)      0           ['tfgpt2_model[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 768)          0           ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 7)            3591        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,837,895\n",
            "Trainable params: 124,837,895\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    hf_gpt2_model = transformers.TFGPT2Model.from_pretrained(checkpoint)\n",
        "    hf_gpt2_model.resize_token_embeddings(len(hf_gpt2_tokenizer))\n",
        "    model = model_top(hf_gpt2_model)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-aZgu9qGb6i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-aZgu9qGb6i",
        "outputId": "ab50b35a-ebd5-48b6-d1e3-a0e3d62359e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7ee8a5cb3700>,\n",
              " <keras.engine.input_layer.InputLayer at 0x7ee8a5bb77c0>,\n",
              " <transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model at 0x7ee800aa65b0>,\n",
              " <keras.layers.pooling.average_pooling1d.AveragePooling1D at 0x7ee8a5769670>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7ee8a5d5d8b0>,\n",
              " <keras.layers.core.dense.Dense at 0x7ee8a5c96eb0>,\n",
              " <keras.layers.regularization.dropout.Dropout at 0x7ee8a5bb7550>,\n",
              " <keras.layers.core.dense.Dense at 0x7ee8a5778370>]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4MEFDK4b0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc4MEFDK4b0b",
        "outputId": "6f1fcfd1-13a6-4e4e-edda-4f38916d81ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
            "                                                                                                  \n",
            " tfgpt2_model (TFGPT2Model)     TFBaseModelOutputWi  124440576   ['input_1[0][0]',                \n",
            "                                thPastAndCrossAtten               'input_2[0][0]']                \n",
            "                                tions(last_hidden_s                                               \n",
            "                                tate=(None, 512, 76                                               \n",
            "                                8),                                                               \n",
            "                                 past_key_values=((                                               \n",
            "                                2, None, 12, 512, 6                                               \n",
            "                                4),                                                               \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64),                                                             \n",
            "                                 (2, None, 12, 512,                                               \n",
            "                                 64)),                                                            \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None,                                                \n",
            "                                cross_attentions=No                                               \n",
            "                                ne)                                                               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 1, 768)      0           ['tfgpt2_model[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 768)          0           ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 512)          393728      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 512)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 7)            3591        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,837,895\n",
            "Trainable params: 397,319\n",
            "Non-trainable params: 124,440,576\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.layers[2].trainable = False\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ky67OTAeUjwf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky67OTAeUjwf",
        "outputId": "395cd630-d195-4787-8ac4-5db06d6e020b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Tue Aug  9 00:59:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
            "| N/A   42C    P0    65W / 300W |  31629MiB / 32510MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   1  Tesla V100-SXM2...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
            "| N/A   35C    P0    66W / 300W |  31463MiB / 32510MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   2  Tesla V100-SXM2...  Off  | 00000000:86:00.0 Off |                    0 |\n",
            "| N/A   36C    P0    64W / 300W |  31463MiB / 32510MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "|   3  Tesla V100-SXM2...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
            "| N/A   45C    P0    67W / 300W |  31463MiB / 32510MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      8944      G   /usr/bin/X                         63MiB |\n",
            "|    0   N/A  N/A      9058      G   /usr/bin/gnome-shell               20MiB |\n",
            "|    0   N/A  N/A    259481      C   ...da/envs/ds6050/bin/python    31539MiB |\n",
            "|    1   N/A  N/A      8944      G   /usr/bin/X                         22MiB |\n",
            "|    1   N/A  N/A    259481      C   ...da/envs/ds6050/bin/python    31435MiB |\n",
            "|    2   N/A  N/A      8944      G   /usr/bin/X                         22MiB |\n",
            "|    2   N/A  N/A    259481      C   ...da/envs/ds6050/bin/python    31435MiB |\n",
            "|    3   N/A  N/A      8944      G   /usr/bin/X                         22MiB |\n",
            "|    3   N/A  N/A    259481      C   ...da/envs/ds6050/bin/python    31435MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-5Rt2wQFYfuE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5Rt2wQFYfuE",
        "outputId": "704ad525-840a-461d-8397-8a4ab34718f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-09 00:59:01.985272: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_3\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "input: \"Placeholder/_1\"\n",
            "input: \"Placeholder/_2\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_DOUBLE\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 36024\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 512\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: 7\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "experimental_type {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_DATASET\n",
            "    args {\n",
            "      type_id: TFT_PRODUCT\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_INT32\n",
            "        }\n",
            "      }\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_INT32\n",
            "        }\n",
            "      }\n",
            "      args {\n",
            "        type_id: TFT_TENSOR\n",
            "        args {\n",
            "          type_id: TFT_DOUBLE\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "INFO:tensorflow:batch_all_reduce: 150 all-reduces with algorithm = nccl, num_packs = 1\n",
            "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:batch_all_reduce: 150 all-reduces with algorithm = nccl, num_packs = 1\n",
            "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-09 00:59:59.420631: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n",
            "2022-08-09 01:00:00.204518: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n",
            "2022-08-09 01:00:00.784194: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n",
            "2022-08-09 01:00:01.277272: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 94/901 [==>...........................] - ETA: 4:43 - loss: 2.2102 - accuracy: 0.2417"
          ]
        }
      ],
      "source": [
        "checkpoint_filepath = './tmp/checkpoint'\n",
        "\n",
        "with strategy.scope():\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=1,\n",
        "        mode=\"auto\",\n",
        "    )\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((\n",
        "        (train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask']),\n",
        "        y_train))\n",
        "    \n",
        "    ds = ds.shuffle(buffer_size=512)\n",
        "    ds_len = len(ds)\n",
        "    train_size = np.floor(0.8*ds_len)\n",
        "    ds_train = ds.take(train_size).batch(32)\n",
        "    ds_val = ds.skip(train_size).batch(32)\n",
        "    \n",
        "    history = model.fit(ds_train,\n",
        "                        validation_data=ds_val,\n",
        "                        epochs=8,\n",
        "                        batch_size=max_len,\n",
        "                        callbacks=[model_checkpoint_callback, early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dSfG36ZOUjiW",
      "metadata": {
        "id": "dSfG36ZOUjiW"
      },
      "outputs": [],
      "source": [
        "# plotting training history\n",
        "history_df = pd.DataFrame(np.array([history.history['accuracy'], history.history['loss']]).T, columns = ['accuracy', 'loss'])\n",
        "history_df = history_df.reset_index().rename(columns = {'index': 'epoch'})\n",
        "history_df['epoch'] = history_df['epoch'] + 1\n",
        "history_df = pd.melt(history_df, id_vars = 'epoch', value_vars = ['accuracy', 'loss'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize = (14,8))\n",
        "sns.lineplot(x = 'epoch', y = 'value', hue = 'variable', data = history_df);\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Epoch', fontsize = 12);\n",
        "ax.set_ylabel(''); \n",
        "ax.set_title('Accuracy and Loss with Training, GPT-2', loc = 'left', fontsize = 20); \n",
        "#ax.xaxis.set_ticklabels(['','1','','','','2','','','','3']); \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0I6aFJ4KPnvn",
      "metadata": {
        "id": "0I6aFJ4KPnvn"
      },
      "outputs": [],
      "source": [
        "## creating confusion matrices\n",
        "## predict w/ model for evaluation\n",
        "predict_train_data = model.predict([train_encodings['input_ids'], \n",
        "                                    train_encodings['attention_mask']], \n",
        "                                   batch_size = 4)\n",
        "predict_test_data  = model.predict([test_encodings['input_ids'],  \n",
        "                                    test_encodings['attention_mask']])\n",
        "\n",
        "pred_train_data = np.argmax(predict_train_data, axis = 1)\n",
        "train_cm = confusion_matrix(np.argmax(y_train, axis = 1), pred_train_data)\n",
        "\n",
        "pred_test_data = np.argmax(predict_test_data, axis = 1)\n",
        "test_cm = confusion_matrix(np.argmax(y_test, axis = 1), pred_test_data)\n",
        "\n",
        "## predict with non-fine-tuned model for comparison\n",
        "model_untr = model_top(hf_gpt2_model)\n",
        "untr_pred_train = model_untr.predict([train_encodings['input_ids'], \n",
        "                                      train_encodings['attention_mask']], \n",
        "                                     batch_size = 4)\n",
        "untr_pred_test  = model_untr.predict([test_encodings['input_ids'], \n",
        "                                      test_encodings['attention_mask']])\n",
        "\n",
        "untr_train_cm = confusion_matrix(np.argmax(y_train, axis = 1), \n",
        "                                 np.argmax(untr_pred_train, axis = 1))\n",
        "untr_test_cm = confusion_matrix(np.argmax(y_test, axis = 1), \n",
        "                                np.argmax(untr_pred_test, axis = 1))\n",
        "\n",
        "labels = list(df['topic'].unique())\n",
        "labels.sort()\n",
        "x_labs = labels\n",
        "labels.sort(reverse = True)\n",
        "y_labs = labels\n",
        "\n",
        "## function for visualizing confusion matrices\n",
        "def plot_cm(cm, title = 'Confusion Matrix'):\n",
        "  fig = plt.figure(figsize = (14,8))\n",
        "  ax = sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Blues');\n",
        "  # labels, title and ticks\n",
        "  ax.set_xlabel('Predicted category', fontsize = 12);\n",
        "  ax.set_ylabel('Actual category', fontsize = 12); \n",
        "  ax.set_title(title, fontsize = 20); \n",
        "  ax.xaxis.set_ticklabels(x_labs, fontsize = 8); \n",
        "  ax.yaxis.set_ticklabels(y_labs, fontsize = 8);\n",
        "\n",
        "  ax.set_facecolor('w')\n",
        "  fig.set_facecolor('w')\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(train_cm, 'GPT-2 Confusion Matrix, Training Data')"
      ],
      "metadata": {
        "id": "6dF_Ce4iQqJl"
      },
      "id": "6dF_Ce4iQqJl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(test_cm, 'GPT-2 Confusion Matrix, Testing Data')"
      ],
      "metadata": {
        "id": "fr54i_evGqHC"
      },
      "id": "fr54i_evGqHC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(untr_train_cm, 'GPT-2 Confusion Matrix, Training Data (not fine-tuned)')"
      ],
      "metadata": {
        "id": "2b2w_wBYGKsV"
      },
      "id": "2b2w_wBYGKsV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(untr_test_cm, 'GPT-2 Confusion Matrix, Testing Data (not fine-tuned)')"
      ],
      "metadata": {
        "id": "DUHBTl7cGUbb"
      },
      "id": "DUHBTl7cGUbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wuescr3icFCW",
      "metadata": {
        "id": "wuescr3icFCW"
      },
      "outputs": [],
      "source": [
        "# see f1 scores\n",
        "# threshold is just median/mean rounded up to the nearest 0.15\n",
        "f1_metric = tfa.metrics.F1Score(num_classes = 7, threshold = 0.15)\n",
        "f1_metric.update_state(y_train, predict_train_data)\n",
        "train_f1 = f1_metric.result()\n",
        "f1_metric.update_state(y_test, predict_test_data)\n",
        "test_f1 = f1_metric.result()\n",
        "\n",
        "# turn to dataframe\n",
        "train_f1 = pd.Series(train_f1.numpy()).reset_index().rename(columns = {'index': 'category', 0: 'f1'})\n",
        "train_f1['type'] = 'train'\n",
        "test_f1  = pd.Series(test_f1.numpy()).reset_index().rename(columns  = {'index': 'category', 0: 'f1'})\n",
        "test_f1['type']  = 'test'\n",
        "\n",
        "gpt2_f1 = pd.concat([train_f1, test_f1]).reset_index(drop = True)\\\n",
        "            .replace({'category': {t: idx for idx, t in zip(sorted(df['topic'].unique()), range(7))}})\\\n",
        "            .sort_values(by = ['category', 'type'], ascending = False)\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize = (14,8))\n",
        "# can't get it to sort alphabetically for some reason\n",
        "ax = sns.barplot(x = 'category', y = 'f1', hue = 'type', data = gpt2_f1, order = list(set(gpt2_f1.category)));\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Category', fontsize = 12);\n",
        "ax.set_ylabel('F1 Score'); \n",
        "ax.set_title('F1 Score in Training and Testing Data, GPT-2', fontsize = 20); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.set_ylim([0, 1]);\n",
        "\n",
        "ax.set_facecolor('w')\n",
        "fig.set_facecolor('w')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see f1 scores for non-fine tuned model\n",
        "# threshold is just median/mean rounded up to the nearest 0.15\n",
        "f1_metric_untr = tfa.metrics.F1Score(num_classes = 7, threshold = 0.15)\n",
        "f1_metric_untr.update_state(y_train, untr_pred_train)\n",
        "untr_train_f1 = f1_metric_untr.result()\n",
        "f1_metric_untr.update_state(y_test,  untr_pred_test)\n",
        "untr_test_f1 = f1_metric_untr.result()\n",
        "\n",
        "# turn to dataframe\n",
        "untr_train_f1 = pd.Series(untr_train_f1.numpy()).reset_index()\\\n",
        "                  .rename(columns = {'index': 'category', 0: 'f1'})\n",
        "untr_train_f1['type'] = 'train'\n",
        "untr_test_f1  = pd.Series(untr_test_f1.numpy()).reset_index()\\\n",
        "                  .rename(columns  = {'index': 'category', 0: 'f1'})\n",
        "untr_test_f1['type']  = 'test'\n",
        "\n",
        "untr_gpt2_f1 = pd.concat([untr_train_f1, untr_test_f1]).reset_index(drop = True)\\\n",
        "                 .replace({'category': {t: idx for idx, t in zip(sorted(df['topic'].unique()), range(7))}})\\\n",
        "                 .sort_values(by = ['category', 'type'], ascending = False)\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize = (14,8))\n",
        "# can't get it to sort alphabetically for some reason\n",
        "ax = sns.barplot(x = 'category', y = 'f1', hue = 'type', data = untr_gpt2_f1, order = list(set(untr_gpt2_f1.category)));\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Category', fontsize = 12);\n",
        "ax.set_ylabel('F1 Score'); \n",
        "ax.set_title('F1 Score in Training and Testing Data, GPT-2 (not fine-tuned)', fontsize = 20); \n",
        "ax.xaxis.set_ticklabels(labels); \n",
        "ax.set_ylim([0, 1]);\n",
        "\n",
        "ax.set_facecolor('w')\n",
        "fig.set_facecolor('w')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UsH_st9yHNLU"
      },
      "id": "UsH_st9yHNLU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualizing model architecture\n",
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model, to_file='model_gpt2_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "t5oO7uMHHZ_1"
      },
      "id": "t5oO7uMHHZ_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model_untr, to_file='model_untr_gpt2_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "gQ9icB5BHemH"
      },
      "id": "gQ9icB5BHemH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy_of_gpt2_model_(full)_dataapi.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}