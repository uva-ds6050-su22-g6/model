{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "17ed0b92"
      },
      "source": [
        "\n",
        "# DS6050 - Group 6\n",
        "* Andrej Erkelens <wsw3fa@virginia.edu>\n",
        "* Robert Knuuti <uqq5zz@virginia.edu>\n",
        "* Khoi Tran <kt2np@virginia.edu>\n",
        "\n",
        "## Abstract\n",
        "English is a verbose language with over 69% redundancy in its construction, and as a result, individuals only need to identify important details to comprehend an intended message.\n",
        "While there are strong efforts to quantify the various elements of language, the average individual can still comprehend a written message that has errors, either in spelling or in grammar.\n",
        "The emulation of the effortless, yet obscure task of reading, writing, and understanding language is the perfect challenge for the biologically-inspired methods of deep learning.\n",
        "Most language and text related problems rely upon finding high-quality latent representations to understand the task at hand. Unfortunately, efforts to overcome such problems are limited to the data and computation power available to individuals; data availability often presents the largest problem, with small, specific domain tasks often proving to be limiting.\n",
        "Currently, these tasks are often aided or overcome by pre-trained large language models (LLMs), designed by large corporations and laboratories.\n",
        "Fine-tuning language models on domain-specific vocabulary with small data sizes still presents a challenge to the language community, but the growing availability of LLMs to augment such models alleviates the challenge.\n",
        "This paper explores different techniques to be applied on existing language models (LMs), built highly complex Deep Learning models, and investigates how to fine-tune these models, such that a pre-trained model is used to enrich a more domain-specific model that may be limited in textual data.\n",
        "\n",
        "## Project Objective\n",
        "\n",
        "We are aiming on using several small domain specific language tasks, particularly classification tasks.\n",
        "We aim to take at least two models, probably BERT and distill-GPT2 as they seem readily available on HuggingFace and TensorFlow's model hub.\n",
        "We will iterate through different variants of layers we fine tune and compare these results with fully trained models, and ideally find benchmarks already in academic papers on all of the datasets.\n",
        "\n",
        "We aim to optimize compute efficiency and also effectiveness of the model on the given dataset. Our goal is to find a high performing and generalizable method for our fine tuning process and share this in our paper.\n"
      ],
      "id": "17ed0b92"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f8971652",
        "outputId": "e7181664-23fe-43cb-be3b-9d0e243cefa7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(0)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosave disabled\n"
          ]
        }
      ],
      "source": [
        "%autosave 0"
      ],
      "id": "f8971652"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M7fV1R567PdR"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-text tokenizers transformers"
      ],
      "id": "M7fV1R567PdR"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CUp8f1yr760r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ],
      "id": "CUp8f1yr760r"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA3eBwuu-Dlf",
        "outputId": "c18d92a0-1431-442d-f6e9-096be40156af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "vA3eBwuu-Dlf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8jU9Dao-ZL5",
        "outputId": "0be3a4b5-3a02-4fc7-c744-2c92f5ccfbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ds6050/git\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ds6050/git/"
      ],
      "id": "j8jU9Dao-ZL5"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6eb2e492"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tokenizers\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from tensorflow import keras\n",
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "df = pd.read_feather(\"data-extractor/data/dataset.feather\")#.set_index('index')\n",
        "df['topic'] = df['topic'].str.split('.').str[0]\n",
        "df_train = df.sample(frac = 0.8)\n",
        "df_test = df.drop(df_train.index)"
      ],
      "id": "6eb2e492"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ae8d1c35"
      },
      "outputs": [],
      "source": [
        "features = 'content' # feature for the future - add all the datasets ['categories', 'summary', 'content']\n",
        "label = 'topic'"
      ],
      "id": "ae8d1c35"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aa1994aa"
      },
      "outputs": [],
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()"
      ],
      "id": "aa1994aa"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JkZm9g-J6Cge"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "# (45030, 7): 7 different topics\n",
        "y_ = ohe.fit_transform(df['topic'].values.reshape(-1,1)).toarray()"
      ],
      "id": "JkZm9g-J6Cge"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k98DISb3X2Yr",
        "outputId": "03e9e63c-75d7-4ade-85c6-a8e89058dcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2Model.\n",
            "\n",
            "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "#max_len = 512\n",
        "checkpoint = 'gpt2'\n",
        "hf_gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(checkpoint, add_prefix_space=True)\n",
        "hf_gpt2_model = transformers.TFGPT2Model.from_pretrained(checkpoint)\n",
        "# hf_gpt2_model = transformers.GPT2ForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# add for gpt2 padding\n",
        "if hf_gpt2_tokenizer.pad_token is None:\n",
        "    hf_gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "hf_gpt2_model.resize_token_embeddings(len(hf_gpt2_tokenizer))\n",
        "\n",
        "# batch encoding\n",
        "encodings = hf_gpt2_tokenizer.batch_encode_plus(list(df.summary.values), \n",
        "                                                return_tensors='tf', \n",
        "                                                padding='max_length',\n",
        "                                                #add_special_tokens=True,\n",
        "                                                max_length=None,\n",
        "                                                truncation=True)"
      ],
      "id": "k98DISb3X2Yr"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QukShKq2lnak"
      },
      "outputs": [],
      "source": [
        "train_encodings = hf_gpt2_tokenizer.batch_encode_plus(list(df_train.summary.values), \n",
        "                                                      return_tensors='tf', \n",
        "                                                      padding='max_length',\n",
        "                                                      max_length=None,\n",
        "                                                      truncation=True)\n",
        "\n",
        "test_encodings  = hf_gpt2_tokenizer.batch_encode_plus(list(df_test.summary.values), \n",
        "                                                      return_tensors='tf', \n",
        "                                                      padding='max_length',\n",
        "                                                      max_length=None,\n",
        "                                                      truncation=True)\n",
        "\n",
        "y_train = ohe.fit_transform(df_train['topic'].values.reshape(-1,1)).toarray()\n",
        "y_test  = ohe.fit_transform(df_test['topic'].values.reshape(-1,1)).toarray()"
      ],
      "id": "QukShKq2lnak"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rIYVgwNF1KSI"
      },
      "outputs": [],
      "source": [
        "def model_top(pretr_model):\n",
        "  input_ids = tf.keras.Input(shape=(1024,), dtype='int32')\n",
        "  attention_mask = tf.keras.Input(shape=(1024,), dtype='int32')\n",
        "\n",
        "  output = pretr_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "  # output = pretr_model([input_ids, attention_mask])\n",
        "  #pooler_output = output[1]\n",
        "  pooler_output = tf.keras.layers.AveragePooling1D(pool_size=1024)(output[0])\n",
        "  flattened_output = tf.keras.layers.Flatten()(pooler_output)\n",
        "  \n",
        "  output = tf.keras.layers.Dense(32, activation='tanh')(flattened_output)\n",
        "  output = tf.keras.layers.Dropout(0.2)(output)\n",
        "\n",
        "  output = tf.keras.layers.Dense(7, activation='softmax')(output)\n",
        "  model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "id": "rIYVgwNF1KSI"
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_top(hf_gpt2_model)"
      ],
      "metadata": {
        "id": "mFmMogLJw2GO"
      },
      "id": "mFmMogLJw2GO",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHO0s2_z2e07",
        "outputId": "e0f5970b-7295-4743-974c-7ec24f069cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " tfgpt2_model (TFGPT2Model)     TFBaseModelOutputWi  124440576   ['input_1[0][0]',                \n",
            "                                thPastAndCrossAtten               'input_2[0][0]']                \n",
            "                                tions(last_hidden_s                                               \n",
            "                                tate=(None, 1024, 7                                               \n",
            "                                68),                                                              \n",
            "                                 past_key_values=((                                               \n",
            "                                2, None, 12, 1024,                                                \n",
            "                                64),                                                              \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64)),                                                           \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None,                                                \n",
            "                                cross_attentions=No                                               \n",
            "                                ne)                                                               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 1, 768)      0           ['tfgpt2_model[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 768)          0           ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           24608       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 32)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 7)            231         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,465,415\n",
            "Trainable params: 124,465,415\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "bHO0s2_z2e07"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaMC57JY4YRv",
        "outputId": "5592e660-df4b-45a8-84ca-32527b5c0d8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7fa5a1d412d0>,\n",
              " <keras.engine.input_layer.InputLayer at 0x7fa5a187cd50>,\n",
              " <transformers.models.gpt2.modeling_tf_gpt2.TFGPT2Model at 0x7fa698df5390>,\n",
              " <keras.layers.pooling.average_pooling1d.AveragePooling1D at 0x7fa5a09acd50>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7fa5a1ab1390>,\n",
              " <keras.layers.core.dense.Dense at 0x7fa5a09795d0>,\n",
              " <keras.layers.regularization.dropout.Dropout at 0x7fa5a0afca90>,\n",
              " <keras.layers.core.dense.Dense at 0x7fa59ff3f3d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model.layers"
      ],
      "id": "XaMC57JY4YRv"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bc4MEFDK4b0b"
      },
      "outputs": [],
      "source": [
        "model.layers[2].trainable = False"
      ],
      "id": "bc4MEFDK4b0b"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMC_xhvt4hfL",
        "outputId": "f8dc8310-02da-4b82-8653-83a4d880a7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " tfgpt2_model (TFGPT2Model)     TFBaseModelOutputWi  124440576   ['input_1[0][0]',                \n",
            "                                thPastAndCrossAtten               'input_2[0][0]']                \n",
            "                                tions(last_hidden_s                                               \n",
            "                                tate=(None, 1024, 7                                               \n",
            "                                68),                                                              \n",
            "                                 past_key_values=((                                               \n",
            "                                2, None, 12, 1024,                                                \n",
            "                                64),                                                              \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64),                                                            \n",
            "                                 (2, None, 12, 1024                                               \n",
            "                                , 64)),                                                           \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None,                                                \n",
            "                                cross_attentions=No                                               \n",
            "                                ne)                                                               \n",
            "                                                                                                  \n",
            " average_pooling1d (AveragePool  (None, 1, 768)      0           ['tfgpt2_model[0][0]']           \n",
            " ing1D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 768)          0           ['average_pooling1d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           24608       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 32)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 7)            231         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,465,415\n",
            "Trainable params: 24,839\n",
            "Non-trainable params: 124,440,576\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "zMC_xhvt4hfL"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky67OTAeUjwf",
        "outputId": "0beec10d-cb73-4293-e34a-8cf64eba55b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug  8 05:08:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    32W /  70W |   2580MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ],
      "id": "Ky67OTAeUjwf"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-5Rt2wQFYfuE"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = './tmp/checkpoint'\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    mode=\"auto\",\n",
        ")"
      ],
      "id": "-5Rt2wQFYfuE"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m11hJnqM4kc0",
        "outputId": "b07c1ad1-45b7-4395-bd83-8c51de5881de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-db12f3629890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     callbacks=[model_checkpoint_callback, early_stopping_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/tfgpt2_model/transformer/h_._1/attn/Softmax' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 661, in <lambda>\n      self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 577, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 606, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 556, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-20-db12f3629890>\", line 7, in <module>\n      callbacks=[model_checkpoint_callback, early_stopping_callback])\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 459, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 728, in run_call_with_unpacked_inputs\n      \"\"\"\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 757, in call\n      outputs = self.transformer(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 728, in run_call_with_unpacked_inputs\n      \"\"\"\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 466, in call\n      for i, (block, layer_past) in enumerate(zip(self.h, past)):\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 470, in call\n      outputs = block(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 255, in call\n      output_attn = self.attn(\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 197, in call\n      attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_tf_gpt2.py\", line 131, in _attn\n      w = stable_softmax(w, axis=-1)\n    File \"/usr/local/lib/python3.7/dist-packages/transformers/tf_utils.py\", line 70, in stable_softmax\n      return tf.nn.softmax(logits=logits + 1e-9, axis=axis, name=name)\nNode: 'model/tfgpt2_model/transformer/h_._1/attn/Softmax'\nOOM when allocating tensor with shape[32,12,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/tfgpt2_model/transformer/h_._1/attn/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_26589]"
          ]
        }
      ],
      "source": [
        "history = model.fit([train_encodings['input_ids'], \n",
        "                     train_encodings['attention_mask']], \n",
        "                    y_train, \n",
        "                    validation_split=.2,\n",
        "                    epochs=4,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[model_checkpoint_callback, early_stopping_callback])"
      ],
      "id": "m11hJnqM4kc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvCeKeivsE_5"
      },
      "outputs": [],
      "source": [
        "#train_labels = df_train['topic']\n",
        "#test_labels  = df_test['topic']\n",
        "\n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
        "#                                                         train_labels))\n",
        "\n",
        "#test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
        "#                                                        test_labels))"
      ],
      "id": "hvCeKeivsE_5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW2PaFMTuHww"
      },
      "outputs": [],
      "source": [
        "#training_args.strategy.scope()"
      ],
      "id": "RW2PaFMTuHww"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQn9a6J2ynks"
      },
      "outputs": [],
      "source": [
        "# hf_gpt2_model.fit(train_dataset)"
      ],
      "id": "yQn9a6J2ynks"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHVXnOWZzVc7"
      },
      "outputs": [],
      "source": [
        "#train_encodings['input_ids']"
      ],
      "id": "mHVXnOWZzVc7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLlGZYeruxnj"
      },
      "outputs": [],
      "source": [
        "#hf_gpt2_model.compile(optimizer='adam',\n",
        "#                      loss='categorical_crossentropy',\n",
        "#                      metrics=['acc'])\n",
        "\n",
        "#hf_gpt2_model.fit(train_encodings['input_ids'])\n",
        "#hf_gpt2_model.fit(train_dataset, epochs=4, validation_data=test_dataset)"
      ],
      "id": "RLlGZYeruxnj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm7HnV7VuEjD"
      },
      "outputs": [],
      "source": [
        "with training_args.strategy.scope():\n",
        "  model = hf_gpt2_model\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "id": "Lm7HnV7VuEjD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPAdQkNTbnBF"
      },
      "source": [
        "### Data Preview"
      ],
      "id": "XPAdQkNTbnBF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZIKcnCiYf-D"
      },
      "outputs": [],
      "source": [
        "for text, label in ds_train.take(5):\n",
        "  print('Text')\n",
        "  print(text)\n",
        "  print('Label')\n",
        "  print(label)"
      ],
      "id": "YZIKcnCiYf-D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmrzbcNs7E_a"
      },
      "outputs": [],
      "source": [
        "\"\"\"## This is currently broken\n",
        "max_len = 384\n",
        "hf_gpt2_tokenizer_bootstrapper = transformers.BertTokenizer.from_pretrained(\"gpt2\")\n",
        "hf_gpt2_model = transformers.TFGPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "save_path = Path(\"data\") / \"models\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "hf_gpt2_tokenizer_bootstrapper.save_pretrained(save_path)\n",
        "hf_gpt2_model.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "gpt2_tokenizer = tokenizers.GPT2TokenizerFast(str(save_path/\"vocab.txt\"), lowercase=True)\n",
        "\n",
        "def tf_hf_gpt2encode(features, label):\n",
        "    x = gpt2_tokenizer.encode(tf.compat.as_str(features), add_special_tokens=True)\n",
        "    y = gpt2_tokenizer.encode(tf.compat.as_str(label), add_special_tokens=True)\n",
        "    return x, y\n",
        "\n",
        "def tf_hf_gpt2encodeds(features, label):\n",
        "    encode = tf.py_function(func=tf_hf_gpt2encode, inp=[features, label], Tout=[tf.int64, tf.int64])\n",
        "    return encode\n",
        "\n",
        "encoded_input = ds_train.batch(256).map(tf_hf_gpt2encodeds)\n",
        "output = transformers.TFGPT2Model(config=transformers.PretrainedConfig.from_json_file(str(save_path/\"config.json\")))\n",
        "hf_gpt2 = output(encoded_input)\"\"\""
      ],
      "id": "DmrzbcNs7E_a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "195d6bac"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "files = [] # Need to explode train_ds to sep files\n",
        "\n",
        "tokenizer = tokenizers.GPT2TokenizerFast(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=True,\n",
        "    strip_accents=True,\n",
        "    lowercase=True,\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files,\n",
        "    vocab_size=10000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    limit_alphabet=1000,\n",
        "    wordpieces_prefix=\"##\",\n",
        ")\n",
        "\n",
        "# Save the files\n",
        "tokenizer.save_model(args.out, args.name)\"\"\""
      ],
      "id": "195d6bac"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "gpt2-model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}